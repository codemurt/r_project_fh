---
title: "LLM Humor Analysis"
author: "Lebedev Egor"
date: "2025-05-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Abstract
This study compares two approaches for generating humorous headlines: direct LLM generation versus Chain-of-Thought (CoT) conceptual blending. Using LLM-as-a-Judge evaluations of 100 generated headlines across four metrics (humor, relevance, creativity, clarity), we analyze whether the more complex CoT approach provides measurable improvements over basic LLM generation.

# 1. Introduction

## 1.1 Research Background

Recent advances in large language models (LLMs) have enabled new approaches to creative text generation. One promising technique is Conceptual Blending through Chain-of-Thought (CoT) prompting, where the model explicitly combines concepts through intermediate reasoning steps.

## 1.2 Research Questions and Hypotheses

Primary research question: Does Conceptual Blending (CoT) produce better humorous headlines than direct LLM generation?

Formal hypotheses:

H0: Conceptual Blending (CoT) does not produce better results than direct LLM generation

H1: Conceptual Blending (CoT) produces better results than direct LLM generation

Secondary questions:

1. Are there differences in specific quality dimensions (humor, relevance, creativity, clarity)?

2. What is the magnitude of any observed differences?

## 1.3 Theoretical Framework

The study draws from:

- Conceptual Blending Theory (Fauconnier & Turner)
- Computational humor generation approaches
- Chain-of-Thought prompting research

# 2. Methods
## 2.1 Data Collection

The dataset contains human evaluations of 100 headline pairs (LLM vs CoT) generated from the same seed concepts.

```{r}
library(tidyverse)
library(ggpubr)
library(rstatix)
library(effectsize)
library(caret)
library(BayesFactor)
library(lme4)
library(ggcorrplot)
```

## 2.2 Dataset Description

```{r}
eval_data <- read_csv("eval_dataset_transformed.csv")

eval_data <- eval_data %>%
  mutate(id = row_number())

# Преобразование в long-формат с сохранением id
long_data <- eval_data %>%
  pivot_longer(
    cols = -c(id, llm_avg, CoT_avg),
    names_to = c("method", "metric"),
    names_sep = "_",
    values_to = "score"
  ) %>%
  mutate(
    method = factor(method, levels = c("llm", "CoT")),
    metric = factor(metric, levels = c("humor", "relevance", "creativity", "clarity"))
  )


# Create dataset for average scores comparison
avg_data <- eval_data %>%
  select(llm_avg, CoT_avg) %>%
  mutate(id = row_number()) %>%
  pivot_longer(
    cols = -id,
    names_to = "method",
    values_to = "score"
  ) %>%
  mutate(method = factor(method, levels = c("llm_avg", "CoT_avg")))
```

Key variables:

- llm_[metric]_score: Direct LLM generation scores
- CoT_[metric]_score: CoT conceptual blending scores
- llm_avg, CoT_avg: Average scores across metrics

## 2.3 Analysis Plan

1. Descriptive statistics and visualizations
2. Normality checks
3. Paired t-tests for overall and metric-specific comparisons
4. Effect size calculations
5. Multiple comparison adjustment

# 3. Results
## 3.1 Descriptive Statistics
```{r}
long_data %>%
  group_by(method) %>%
  get_summary_stats(score, type = "common")

long_data %>%
  group_by(method, metric) %>%
  get_summary_stats(score, type = "common")
```

## 3.2 Data Visualization

```{r}
method_palette <- c("llm" = "#4E79A7",  
                    "CoT" = "#F28E2B")  

metric_palette <- c("humor" = "#E15759",    
                    "relevance" = "#59A14F",
                    "creativity" = "#EDC948",
                    "clarity" = "#76B7B2")  
```

```{r}
ggplot(long_data, aes(x = method, y = score, fill = method)) +
  geom_boxplot(alpha = 0.8) +
  facet_wrap(~metric, nrow = 2) +
  scale_fill_manual(values = method_palette) +
  labs(title = "Score Distribution by Method and Metric",
       x = "Method",
       y = "Score") +
  theme_minimal() +
  theme(legend.position = "none")

avg_data <- avg_data %>%
  mutate(method = str_remove(method, "_avg"))

ggplot(avg_data, aes(x = method, y = score, fill = method)) +
  geom_boxplot(alpha = 0.7) +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_fill_manual(values = method_palette) + 
  labs(title = "Comparison of Average Scores",
       subtitle = "LLM vs CoT Conceptual Blending",
       x = "Method",
       y = "Average Score") +
  theme_minimal()
```

## 3.3 Statistical Analysis

### 3.3.1 Normality Checks

```{r}
shapiro_data <- avg_data %>%
  group_by(method) %>%
  shapiro_test(score)
shapiro_data

t_test_result <- t.test(eval_data$llm_avg, eval_data$CoT_avg, 
                        paired = TRUE, alternative = "less")
t_test_result
```


- Both distributions (LLM and CoT average scores) show statistically significant deviations from normality (p < 0.001 for both)
- The Shapiro-Wilk test statistics (0.74 for LLM, 0.86 for CoT) are both below 0.95, indicating non-normal distributions
- Implication: Non-parametric tests should be preferred for analysis, though with n=100, parametric tests are often considered robust against normality violations

- No significant difference between LLM and CoT average scores (p = 0.769 > 0.05)
- The positive mean difference (0.056) suggests LLM scores are slightly higher, but not statistically significant
- Key finding: The hypothesis that CoT Conceptual Blending produces better headlines is not supported for overall scores

```{r}
ggplot(avg_data, aes(sample = score, color = method)) +
  stat_qq(size = 2.5, alpha = 0.8) + 
  stat_qq_line(linewidth = 1) +
  facet_wrap(~method, nrow = 1) +
  scale_color_manual(values = method_palette) +
  labs(title = "QQ-Plot for Average Scores Normality Check",
       subtitle = "Comparison with Normal Distribution",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none",
        panel.grid.minor = element_blank(),
        strip.text = element_text(face = "bold", size = 12)) +
  geom_text(data = shapiro_data, 
            aes(x = -Inf, y = Inf, 
                label = paste0("Shapiro-Wilk: p = ", format.pval(p, digits = 3))),
            hjust = -0.15, vjust = 1.8, color = "black", size = 5, inherit.aes = FALSE)
```

### 3.3.2 Effect Size

```{r}
# Correct effect size calculation
cohen_d_result <- cohens_d(
  x = eval_data$llm_avg,
  y = eval_data$CoT_avg,
  paired = TRUE,
  ci = 0.95
)
cohen_d_result
```

Negligible effect size (d = 0.07) according to Cohen`s conventions: d < 0.2 = negligible
Practical significance: Even if statistical significance existed, the effect is too small to be practically meaningful

### 3.3.3 Metric-Specific Analysis

```{r}
metric_analysis <- function(metric_name) {
  llm_scores <- eval_data[[paste0("llm_", metric_name, "_score")]]
  cot_scores <- eval_data[[paste0("CoT_", metric_name, "_score")]]
  
  t_test <- t.test(llm_scores, cot_scores, paired = TRUE)
  
  d <- cohens_d(llm_scores, cot_scores, paired = TRUE)
  
  tibble(
    metric = metric_name,
    mean_llm = mean(llm_scores),
    mean_cot = mean(cot_scores),
    mean_diff = mean(llm_scores - cot_scores),
    t_statistic = t_test$statistic,
    df = t_test$parameter,
    p_value = t_test$p.value,
    cohens_d = d$Cohens_d,
    ci_low = d$CI_low,
    ci_high = d$CI_high
  )
}

metric_results <- map_dfr(
  c("humor", "relevance", "creativity", "clarity"),
  metric_analysis
) %>%
  mutate(
    p_adj = p.adjust(p_value, method = "bonferroni"),
    significance = ifelse(p_adj < 0.05, "**", ifelse(p_value < 0.05, "*", ""))
  )

metric_results
```

Insights:

- Basic LLM approach yields more understandable headlines
- Conceptual blending may boost creativity but not consistently
- Both approaches perform similarly on core humor metrics

### 3.3.4 Effect Size Visualization

```{r}
ggplot(metric_results, aes(x = metric, y = cohens_d, 
                           ymin = ci_low, ymax = ci_high,
                           color = metric)) +
  geom_pointrange(size = 1) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Effect Sizes by Metric (Cohen's d with 95% CI)",
       x = "Evaluation Metric",
       y = "Cohen's d (LLM vs CoT)") +
  scale_color_manual(values = metric_palette) +  
  theme_minimal() +
  coord_flip()

long_data %>%
  group_by(method, metric) %>%
  summarise(mean_score = mean(score), .groups = "drop") %>%
  ggplot(aes(x = metric, y = mean_score, fill = method)) +
  geom_col(position = position_dodge(), alpha = 0.8) +
  geom_text(aes(label = round(mean_score, 2)), 
            position = position_dodge(width = 0.9), 
            vjust = -0.5) +
  labs(title = "Mean Scores by Method and Metric",
       x = "Metric",
       y = "Mean Score") +
  scale_fill_manual(values = method_palette) +  
  theme_minimal() +
  ylim(0, max(long_data$score) * 1.1)
```

### 3.3.5 Non-parametric Tests (Robustness Check)

```{r}
wilcox_avg <- wilcox.test(eval_data$llm_avg, eval_data$CoT_avg, 
                          paired = TRUE)
wilcox_avg

dunn_test <- long_data %>%
  dunn_test(score ~ method, p.adjust.method = "bonferroni")
dunn_test
```

V = 2028.5, p-value = 0.08227

There is no statistically significant difference in median average scores between LLM and CoT methods (p = 0.082 > 0.05). The test shows marginal evidence (approaching significance) that LLM performs slightly better, but this difference doesn't reach conventional significance levels. This aligns with our earlier t-test findings. 

Dunn Test (Post-hoc for Friedman): p.adj = 0.0293

When examining all metrics collectively, there's a statistically significant difference between methods (p < 0.05) after multiple comparison adjustment. The negative test statistic (-2.18) indicates LLM outperforms CoT overall, primarily driven by the clarity metric as shown in previous analyses.

### 3.3.6 Rating Consistency Analysis

```{r}
spearman_cor <- cor.test(eval_data$llm_avg, eval_data$CoT_avg, 
                         method = "spearman")
spearman_cor
```
```{r}
cor_df <- eval_data %>%
  select(llm_avg, CoT_avg, ends_with("_score")) %>%
  rename(
    `LLM Avg` = llm_avg,
    `CoT Avg` = CoT_avg,
    `LLM Humor` = llm_humor_score,
    `CoT Humor` = CoT_humor_score,
    `LLM Relevance` = llm_relevance_score,
    `CoT Relevance` = CoT_relevance_score,
    `LLM Creativity` = llm_creativity_score,
    `CoT Creativity` = CoT_creativity_score,
    `LLM Clarity` = llm_clarity_score,
    `CoT Clarity` = CoT_clarity_score
  )

cor_matrix <- cor(cor_df, method = "spearman")

ggcorrplot(cor_matrix, 
           hc.order = TRUE,
           type = "lower",
           lab = TRUE,
           lab_size = 3,
           colors = c("#E46726", "white", "#6D9EC1")) +
  labs(title = "Spearman Correlation Matrix",
       subtitle = "Weak correlation between LLM and CoT performance (ρ = 0.08)") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

rho = 0.075, p-value = 0.456

There is no meaningful correlation (ρ ~ 0.08) between LLM and CoT scores for the same headlines (p = 0.456). This suggests:

- Headlines that scored well with one method didn't necessarily score well with the other
- The two methods produce qualitatively different types of humor
- Performance is headline-specific rather than method-driven

### 3.3.7 Bayesian t-test

```{r}
bayes_ttest <- ttestBF(eval_data$llm_avg, eval_data$CoT_avg, paired = TRUE)
bayes_factor <- exp(bayes_ttest@bayesFactor$bf)

bayes_factor
```
```{r}
bayes_df <- data.frame(
    Hypothesis = c("H0 (No difference)", "H1 (Difference exists)"),
    Probability = c(1/(1+bayes_factor), bayes_factor/(1+bayes_factor))
  )
  
  ggplot(bayes_df, aes(x = Hypothesis, y = Probability, fill = Hypothesis)) +
    geom_col(alpha = 0.8, width = 0.6) +
    geom_text(aes(label = paste0(round(Probability*100, 1), "%")), 
              vjust = -0.5, size = 4) +
    scale_fill_manual(values = c("#4E79A7", "#F28E2B")) +
    labs(title = "Bayesian Probability of Hypotheses",
         subtitle = paste0("BF10 = ", round(bayes_factor, 3)),
         y = "Posterior Probability") +
    theme_minimal() +
    ylim(0, 1) +
    theme(legend.position = "none")
```

Strong evidence in favor of the null hypothesis (BF10 < 1). The data are approximately 7 times more likely (6.94) under H0 (no difference between methods) than H1 (CoT better than LLM). This provides robust confirmation that CoT doesn't improve humor quality.

### 3.3.8 Regression Analysis

```{r}
lmm_model <- lmer(score ~ method * metric + (1 | id), data = long_data)
lmm_summary <- summary(lmm_model)
lmm_anova <- anova(lmm_model)
lmm_summary
summary(lmm_anova)
```
- `methodCoT: -0.08 (t = -0.765)` -> No main effect of method
- `metricrelevance: -0.22 (t = -2.104)` -> Relevance scores significantly lower than humor
- `metriccreativity: 0.38 (t = 3.635)` -> Creativity scores significantly higher than humor
- `methodCoT:metricclarity: -0.17 (t = -1.150)` -> Marginal clarity disadvantage for CoT




# 4. Discussion

## 4.1 Key Findings

1. Primary Hypothesis: The hypothesis that Conceptual Blending (CoT) produces better headlines than basic LLM is not supported (p = `r round(t_test_result$p.value, 3)`, d = `r round(cohen_d_result$Cohens_d, 2)`).
2. Metric-Specific Results:

- Clarity: Basic LLM produces significantly clearer headlines (p = `r round(filter(metric_results, metric == "clarity")$p_adj, 3)`)
- Creativity: Non-significant trend favoring CoT (p = `r round(filter(metric_results, metric == "creativity")$p_adj, 3)`)
- Humor and Relevance: No significant differences

## 4.2 Theoretical Implications

The results suggest that:

- The additional cognitive steps in CoT may not benefit humor generation
- Direct generation may preserve clarity better
- The creative potential of CoT requires further investigation

## 4.3 Limitations
- Sample size of 100 headline pairs
- Potential rater bias in evaluations
- Limited to English language generation

## 4.4 Future Research Directions
- Investigate different CoT prompting strategies
- Examine interaction effects between metrics
- Expand to other languages and text genres

# 5. Conclusion

This study found no significant overall advantage of CoT conceptual blending over direct LLM generation for humorous headlines, with a small but significant clarity advantage for direct generation. The results contribute to our understanding of when complex generation strategies provide measurable benefits.