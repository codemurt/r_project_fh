---
title: "LLM Humor Analysis"
author: "Lebedev Egor"
date: "2025-05-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Abstract
This study compares two approaches for generating humorous headlines: direct LLM generation versus Chain-of-Thought (CoT) conceptual blending. Using LLM-as-a-Judge evaluations of 100 generated headlines across four metrics (humor, relevance, creativity, clarity), we analyze whether the more complex CoT approach provides measurable improvements over basic LLM generation.

# 1. Introduction

## 1.1 Research Background

Recent advances in large language models (LLMs) have enabled new approaches to creative text generation. One promising technique is Conceptual Blending through Chain-of-Thought (CoT) prompting, where the model explicitly combines concepts through intermediate reasoning steps.

## 1.2 Research Questions and Hypotheses

Primary research question: Does Conceptual Blending (CoT) produce better humorous headlines than direct LLM generation?

Formal hypotheses:
H0: Conceptual Blending (CoT) does not produce better results than direct LLM generation
H1: Conceptual Blending (CoT) produces better results than direct LLM generation

Secondary questions:

1. Are there differences in specific quality dimensions (humor, relevance, creativity, clarity)?

2. What is the magnitude of any observed differences?

## 1.3 Theoretical Framework

The study draws from:

- Conceptual Blending Theory (Fauconnier & Turner)
- Computational humor generation approaches
- Chain-of-Thought prompting research

# 2. Methods
## 2.1 Data Collection

The dataset contains human evaluations of 100 headline pairs (LLM vs CoT) generated from the same seed concepts.

```{r}
library(tidyverse)
library(ggpubr)
library(rstatix)
library(effectsize)
library(caret)
```

## 2.2 Dataset Description

```{r}
eval_data <- read_csv("eval_dataset_transformed.csv")

# Convert to long format for analysis
long_data <- eval_data %>%
  pivot_longer(
    cols = -c(llm_avg, CoT_avg),
    names_to = c("method", "metric"),
    names_sep = "_",
    values_to = "score"
  ) %>%
  mutate(
    method = factor(method, levels = c("llm", "CoT")),
    metric = factor(metric, levels = c("humor", "relevance", "creativity", "clarity"))
  )

# Create dataset for average scores comparison
avg_data <- eval_data %>%
  select(llm_avg, CoT_avg) %>%
  mutate(id = row_number()) %>%
  pivot_longer(
    cols = -id,
    names_to = "method",
    values_to = "score"
  ) %>%
  mutate(method = factor(method, levels = c("llm_avg", "CoT_avg")))
```

Key variables:

- llm_[metric]_score: Direct LLM generation scores
- CoT_[metric]_score: CoT conceptual blending scores
- llm_avg, CoT_avg: Average scores across metrics

## 2.3 Analysis Plan

1. Descriptive statistics and visualizations
2. Normality checks
3. Paired t-tests for overall and metric-specific comparisons
4. Effect size calculations
5. Multiple comparison adjustment

# 3. Results
## 3.1 Descriptive Statistics
```{r}
long_data %>%
  group_by(method) %>%
  get_summary_stats(score, type = "common")

long_data %>%
  group_by(method, metric) %>%
  get_summary_stats(score, type = "common")
```

## 3.2 Data Visualization

```{r}
method_palette <- c("llm" = "#4E79A7",  
                    "CoT" = "#F28E2B")  

metric_palette <- c("humor" = "#E15759",    
                    "relevance" = "#59A14F",
                    "creativity" = "#EDC948",
                    "clarity" = "#76B7B2")  
```

```{r}
ggplot(long_data, aes(x = method, y = score, fill = method)) +
  geom_boxplot(alpha = 0.8) +
  facet_wrap(~metric, nrow = 2) +
  scale_fill_manual(values = method_palette) +
  labs(title = "Score Distribution by Method and Metric",
       x = "Method",
       y = "Score") +
  theme_minimal() +
  theme(legend.position = "none")

avg_data <- avg_data %>%
  mutate(method = str_remove(method, "_avg"))

ggplot(avg_data, aes(x = method, y = score, fill = method)) +
  geom_boxplot(alpha = 0.7) +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_fill_manual(values = method_palette) + 
  labs(title = "Comparison of Average Scores",
       subtitle = "LLM vs CoT Conceptual Blending",
       x = "Method",
       y = "Average Score") +
  theme_minimal()
```

## 3.3 Statistical Analysis
### 3.3.1 Normality Checks
```{r}
shapiro_data <- avg_data %>%
  group_by(method) %>%
  shapiro_test(score)
shapiro_data

t_test_result <- t.test(eval_data$llm_avg, eval_data$CoT_avg, 
                        paired = TRUE, alternative = "less")
t_test_result
```


- Both distributions (LLM and CoT average scores) show statistically significant deviations from normality (p < 0.001 for both)
- The Shapiro-Wilk test statistics (0.74 for LLM, 0.86 for CoT) are both below 0.95, indicating non-normal distributions
- Implication: Non-parametric tests should be preferred for analysis, though with n=100, parametric tests are often considered robust against normality violations

- No significant difference between LLM and CoT average scores (p = 0.769 > 0.05)
- The positive mean difference (0.056) suggests LLM scores are slightly higher, but not statistically significant
- Key finding: The hypothesis that CoT Conceptual Blending produces better headlines is not supported for overall scores

### 3.3.2 Effect Size
```{r}
# Correct effect size calculation
cohen_d_result <- cohens_d(
  x = eval_data$llm_avg,
  y = eval_data$CoT_avg,
  paired = TRUE,
  ci = 0.95
)
cohen_d_result
```

Negligible effect size (d = 0.07) according to Cohen`s conventions: d < 0.2 = negligible
Practical significance: Even if statistical significance existed, the effect is too small to be practically meaningful

### 3.3.3 Metric-Specific Analysis

```{r}
metric_analysis <- function(metric_name) {
  # Extract relevant columns
  llm_scores <- eval_data[[paste0("llm_", metric_name, "_score")]]
  cot_scores <- eval_data[[paste0("CoT_", metric_name, "_score")]]
  
  # Paired t-test
  t_test <- t.test(llm_scores, cot_scores, paired = TRUE)
  
  # Effect size
  d <- cohens_d(llm_scores, cot_scores, paired = TRUE)
  
  # Return results
  tibble(
    metric = metric_name,
    mean_llm = mean(llm_scores),
    mean_cot = mean(cot_scores),
    mean_diff = mean(llm_scores - cot_scores),
    t_statistic = t_test$statistic,
    df = t_test$parameter,
    p_value = t_test$p.value,
    cohens_d = d$Cohens_d,
    ci_low = d$CI_low,
    ci_high = d$CI_high
  )
}

# Apply to all metrics
metric_results <- map_dfr(
  c("humor", "relevance", "creativity", "clarity"),
  metric_analysis
) %>%
  mutate(
    p_adj = p.adjust(p_value, method = "bonferroni"),
    significance = ifelse(p_adj < 0.05, "**", ifelse(p_value < 0.05, "*", ""))
  )

metric_results
```

Insights:

- Basic LLM approach yields more understandable headlines
- Conceptual blending may boost creativity but not consistently
- Both approaches perform similarly on core humor metrics

### 3.3.4 Effect Size Visualization

```{r}
ggplot(metric_results, aes(x = metric, y = cohens_d, 
                           ymin = ci_low, ymax = ci_high,
                           color = metric)) +
  geom_pointrange(size = 1) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Effect Sizes by Metric (Cohen's d with 95% CI)",
       x = "Evaluation Metric",
       y = "Cohen's d (LLM vs CoT)") +
  scale_color_manual(values = metric_palette) +  
  theme_minimal() +
  coord_flip()

long_data %>%
  group_by(method, metric) %>%
  summarise(mean_score = mean(score), .groups = "drop") %>%
  ggplot(aes(x = metric, y = mean_score, fill = method)) +
  geom_col(position = position_dodge(), alpha = 0.8) +
  geom_text(aes(label = round(mean_score, 2)), 
            position = position_dodge(width = 0.9), 
            vjust = -0.5) +
  labs(title = "Mean Scores by Method and Metric",
       x = "Metric",
       y = "Mean Score") +
  scale_fill_manual(values = method_palette) +  
  theme_minimal() +
  ylim(0, max(long_data$score) * 1.1)
```


# 4. Discussion
## 4.1 Key Findings

1. Primary Hypothesis: The hypothesis that Conceptual Blending (CoT) produces better headlines than basic LLM is not supported (p = `r round(t_test_result$p.value, 3)`, d = `r round(cohen_d_result$Cohens_d, 2)`).
2. Metric-Specific Results:

- Clarity: Basic LLM produces significantly clearer headlines (p = `r round(filter(metric_results, metric == "clarity")$p_adj, 3)`)
- Creativity: Non-significant trend favoring CoT (p = `r round(filter(metric_results, metric == "creativity")$p_adj, 3)`)
- Humor and Relevance: No significant differences

## 4.2 Theoretical Implications

The results suggest that:

- The additional cognitive steps in CoT may not benefit humor generation
- Direct generation may preserve clarity better
- The creative potential of CoT requires further investigation

## 4.3 Limitations
- Sample size of 100 headline pairs
- Potential rater bias in evaluations
- Limited to English language generation

## 4.4 Future Research Directions
- Investigate different CoT prompting strategies
- Examine interaction effects between metrics
- Expand to other languages and text genres

# 5. Conclusion

This study found no significant overall advantage of CoT conceptual blending over direct LLM generation for humorous headlines, with a small but significant clarity advantage for direct generation. The results contribute to our understanding of when complex generation strategies provide measurable benefits.